${template main-}
${file 'lexer.go'-}
${call go.header-}
${call lexer-}
${end-}
${file 'lexer_tables.go'-}
${call go.header-}
${call lexerTables-}
${end-}
${end}

${template lexer-}
package ${self->go.package()}
${if syntax.lexerStates.size() > 1-}
// Lexer states.
const (
${foreach state in syntax.lexerStates-}
	${state->stateId()} = ${state.index}
${end-}
)
${end-}
${call onBeforeLexer-}
${call lexerType-}
${call lexerInit-}
${call lexerNext-}
${call lexerPos-}
${if opts.tokenLine-}
${call lexerLine-}
${end-}
${if opts.tokenColumn-}
${call lexerColumn-}
${end-}
${call lexerText-}
${call lexerValue-}
${call lexerRewind-}
${call onAfterLexer-}
${end}

${template lexerType}
// Lexer uses a generated DFA to scan through a utf-8 encoded input string. If
// the string starts with a BOM character, it gets skipped.
type Lexer struct {
	source string

	ch          rune // current character, -1 means EOI
	offset      int  // character offset
	tokenOffset int  // last token offset
${if opts.tokenLine-}
	line        int  // current line number (1-based)
	tokenLine   int  // last token line
${end-}
${if opts.tokenLineOffset || opts.tokenColumn-}
	lineOffset  int  // current line offset
${end-}
${if opts.tokenColumn-}
	tokenColumn int  // last token column (in bytes)
${end-}
	scanOffset  int  // scanning offset
	value       interface{}

	State int // lexer state, modifiable
${call stateVars-}
}
${end}

${template lexerInit}
var bomSeq = "\xef\xbb\xbf"

// Init prepares the lexer l to tokenize source by performing the full reset
// of the internal state.
func (l *Lexer) Init(source string) {
	l.source = source

	l.ch = 0
	l.offset = 0
	l.tokenOffset = 0
${if opts.tokenLine-}
	l.line = 1
	l.tokenLine = 1
${end-}
${if opts.tokenLineOffset || opts.tokenColumn-}
	l.lineOffset = 0
${end-}
${if opts.tokenColumn-}
	l.tokenColumn = 1
${end-}
	l.State = 0
${call initStateVars-}

	if "strings".HasPrefix(source, bomSeq) {
		l.offset += len(bomSeq)
	}

	l.rewind(l.offset)
}
${end}

${template lexerNext}
// Next finds and returns the next token in l.source. The source end is
// indicated by Token.EOI.
//
// The token text can be retrieved later by calling the Text() method.
func (l *Lexer) Next() ${self->go_token.tokenTypeRef()} {
${call onBeforeNext-}
${if !self->canInlineLexerRules() || syntax.lexerRules.exists(r|r->isSpace())-}
restart:
${end-}
${if opts.tokenLine-}
	l.tokenLine = l.line
${end-}
${if opts.tokenColumn-}
	l.tokenColumn = l.offset-l.lineOffset+1
${end-}
	l.tokenOffset = l.offset

	state := tmStateMap[l.State]
${if self->useCustomMap()-}
	hash := uint32(0)
${end-}
${if lex.backtracking.length > 0-}
	${self->backupVar()} := -1
	var backupOffset int
${if self->useCustomMap()-}
	backupHash := hash
${end-}
${end-}
	for state >= 0 {
		var ch int
		if uint(l.ch) < tmRuneClassLen {
			ch = int(tmRuneClass[l.ch])
		} else if l.ch < 0 {
			state = int(tmLexerAction[state*tmNumClasses])
${if lex.backtracking.length > 0-}
			if state > tmFirstRule && state < 0 {
				state = (-1 - state) * 2
				${self->backupVar()} = tmBacktracking[state]
				backupOffset = l.offset
${if self->useCustomMap()-}
				backupHash = hash
${end-}
				state = tmBacktracking[state+1]
			}
${end-}
			continue
		} else {
${if lex.char2no.length >= 2048-}
			ch = mapRune(l.ch)
${else-}
			ch = 1
${end-}
		}
		state = int(tmLexerAction[state*tmNumClasses+ch])
		if state > tmFirstRule {
${if lex.backtracking.length > 0-}
			if state < 0 {
				state = (-1 - state) * 2
				${self->backupVar()} = tmBacktracking[state]
				backupOffset = l.offset
${if self->useCustomMap()-}
				backupHash = hash
${end-}
				state = tmBacktracking[state+1]
			}
${end-}
${if self->useCustomMap()-}
			hash = hash*uint32(31) + uint32(l.ch)

${end-}
${if opts.tokenLine-}
			if l.ch == '\n' {
				l.line++
${if opts.tokenLineOffset || opts.tokenColumn-}
				l.lineOffset = l.offset
${end-}
			}

${end-}
			// Scan the next character.
			// Note: the following code is inlined to avoid performance implications.
			l.offset = l.scanOffset
			if l.offset < len(l.source) {
				r, w := rune(l.source[l.offset]), 1
				if r >= 0x80 {
					// not ASCII
					r, w = "unicode/utf8".DecodeRuneInString(l.source[l.offset:])
				}
				l.scanOffset += w
				l.ch = r
			} else {
				l.ch = -1 // EOI
			}
		}
	}

${if self->canInlineLexerRules()-}
	${self->go_token.tokenVar()} := ${self->go_token.tokenTypeRef()}(tmFirstRule - state)
${else-}
	rule := tmFirstRule - state
${end-}
${if lex.backtracking.length > 0-}
recovered:
${end-}
${if self->classRules().exists(it|it->classHasInstances())-}
	switch ${self->canInlineLexerRules() ? self->go_token.tokenVar() : 'rule'} {
${foreach classRule in self->classRules().select(it|it->classHasInstances())-}
${if self->canInlineLexerRules()-}
	case ${classRule.symbol->go_token.tokenRef()}:
${else-}
	case ${classRule.index+2}:
${end-}
		hh := hash & ${classRule->rangeSwitchSize() - 1}
		switch hh {
${foreach instance in classRule->classInstances().groupBy(it|util.rangedHash(it.regexp.constantValue, classRule->rangeSwitchSize())).sort(it|util.rangedHash((it is java.util.List ? it[0] : it).regexp.constantValue, classRule->rangeSwitchSize()))-}
${if instance is java.util.List-}
		case ${util.rangedHash(instance[0].regexp.constantValue, classRule->rangeSwitchSize())}:
${foreach i in instance.sort(it|it.regexp.constantValue)-}
${call instanceToRuleIf(i.regexp.constantValue, i)-}
${end-}
${else-}
		case ${util.rangedHash(instance.regexp.constantValue, classRule->rangeSwitchSize())}:
${call instanceToRuleIf(instance.regexp.constantValue, instance)-}
${end-}
${end-}
		}
${end-}
	}
${end-}
${if self->canInlineLexerRules()-}
	switch ${self->go_token.tokenVar()} {
	case ${self->go_token.invalidTokenRef()}:
${call handleInvalidToken-}
${if list = syntax.lexerRules.select(r|r->isSpace()).collectUnique(r|r.symbol.index), list.size() > 0-}
	case ${list->util.join(', ')}:
		goto restart
${end-}
	}
${else-}

	${self->go_token.tokenVar()} := tmToken[rule]
	var space bool
${if syntax.lexerRules.exists(r|r.code)-}
	switch rule {
	case 0:
${call handleInvalidToken-}
${foreach rule in syntax.lexerRules.select(r|r.code)-}
	case ${rule.index+2}: // ${rule.symbol.name}: /${rule.regexp.text}/
${rule.code-}
${end-}
	}
${else-}
	if rule == 0 {
${call handleInvalidToken-}
	}
${end-}
	if space {
		goto restart
	}
${end-}
${call onAfterNext-}
	return ${self->go_token.tokenVar()}
}
${end}

${template lexerPos}
// Pos returns the start and end positions of the last token returned by Next().
func (l *Lexer) Pos() (start, end int) {
	start = l.tokenOffset
	end = l.offset
	return
}
${end}

${template lexerLine}
// Line returns the line number of the last token returned by Next() (1-based).
func (l *Lexer) Line() int {
	return l.tokenLine
}
${end}

${template lexerColumn}
// Column returns the column of the last token returned by Next() (in bytes, 1-based).
func (l *Lexer) Column() int {
	return l.tokenColumn
}
${end}

${template lexerText}
// Text returns the substring of the input corresponding to the last token.
func (l *Lexer) Text() string {
	return l.source[l.tokenOffset:l.offset]
}
${end}

${template lexerValue}
// Value returns the value associated with the last returned token.
func (l *Lexer) Value() interface{} {
	return l.value
}
${end}

${template lexerRewind}
// rewind can be used in lexer actions to accept a portion of a scanned token, or to include
// more text into it.
func (l *Lexer) rewind(offset int) {
${if opts.tokenLine-}
	if offset < l.offset {
		l.line -= "strings".Count(l.source[offset:l.offset], "\n")
	} else {
		if offset > len(l.source) {
			offset = len(l.source)
		}
		l.line += "strings".Count(l.source[l.offset:offset], "\n")
	}
${if opts.tokenLineOffset || opts.tokenColumn-}
	l.lineOffset = 1 + "strings".LastIndexByte(l.source[:offset], '\n')
${end-}

${end-}
	// Scan the next character.
	l.scanOffset = offset
	l.offset = offset
	if l.offset < len(l.source) {
		r, w := rune(l.source[l.offset]), 1
		if r >= 0x80 {
			// not ASCII
			r, w = "unicode/utf8".DecodeRuneInString(l.source[l.offset:])
		}
		l.scanOffset += w
		l.ch = r
	} else {
		l.ch = -1 // EOI
	}
}
${end}

${template handleInvalidToken-}
${if lex.backtracking.length > 0-}
		if ${self->backupVar()} >= 0 {
${if self->canInlineLexerRules()-}
			${self->go_token.tokenVar()} = ${self->go_token.tokenTypeRef()}(backupToken)
${else-}
			rule = backupRule
${end-}
${if self->useCustomMap()-}
			hash = backupHash
${end-}
			l.rewind(backupOffset)
		} else if l.offset == l.tokenOffset {
			l.rewind(l.scanOffset)
		}
${if self->canInlineLexerRules()-}
		if ${self->go_token.tokenVar()} != ${self->go_token.invalidTokenRef()} {
${else-}
		if rule != 0 {
${end-}
			goto recovered
		}
${else-}
		if l.offset == l.tokenOffset {
			l.rewind(l.scanOffset)
		}
${end-}
${end}

${template instanceToRuleIf(val,instanceRule)-}
			if hash == 0x${util.hashHex(val)} && "${util.escape(val)}" == l.source[l.tokenOffset:l.offset] {
${if self->canInlineLexerRules()-}
				${self->go_token.tokenVar()} = ${instanceRule.symbol->go_token.tokenRef()}
${else-}
				rule = ${instanceRule.index+2}
${end-}
				break
			}
${end}

${template lexerTables-}
package ${self->go.package()}

const tmNumClasses = ${lex.nchars}

${if lex.char2no.length >= 2048-}
type mapRange struct {
	lo         rune
	hi         rune
	defaultVal ${self->runeClassType()}
	val        []${self->runeClassType()}
}

func mapRune(c rune) int {
	lo := 0
	hi := len(tmRuneRanges)
	for lo < hi {
		m := lo + (hi-lo)/2
		r := tmRuneRanges[m]
		if c < r.lo {
			hi = m
		} else if c >= r.hi {
			lo = m + 1
		} else {
			i := int(c - r.lo)
			if i < len(r.val) {
				return int(r.val[i])
			}
			return int(r.defaultVal)
		}
	}
	return 1
}

// Latin-1 characters.
var tmRuneClass = []${self->runeClassType()}{
${util.formatEx(util.head(lex.char2no, 256), '\t', 79)-}
}

const tmRuneClassLen = 256
const tmFirstRule = ${-1 - (lex.backtracking.length / 2)}

var tmRuneRanges = []mapRange{
${foreach r in util.packAsMapRanges(util.tail(lex.char2no, 256), 256)-}
	{${r.lo}, ${r.hi}, ${r.defaultVal}, ${if r.val}[]${self->runeClassType()}{
${util.formatEx(r.val, '\t\t', 78)-}
	}${else}nil${end}},
${end-}
}
${else-}
var tmRuneClass = []${self->runeClassType()}{
${util.formatEx(lex.char2no, '\t', 79)-}
}

const tmRuneClassLen = ${lex.char2no.length}
const tmFirstRule = ${-1 - (lex.backtracking.length / 2)}
${end-}

var tmStateMap = []int{
${util.formatEx(lex.groupset, '\t', 79)-}
}

${if !self->canInlineLexerRules()-}
var tmToken = []${self->go_token.tokenTypeRef()}{
${util.formatEx(syntax.lexerRuleTokens, '\t', 79)-}
}

${end-}
${if change = self->canInlineLexerRules()
              ? syntax.inlineLexerRules(-1 - (lex.backtracking.length / 2), lex.change)
              : lex.change,
     true -}
var tmLexerAction = []int${util.bitsForElement(change)}{
${util.formatEx(change, '\t', 79)-}
}

${end-}
${if backtracking = self->canInlineLexerRules()
                   ? syntax.inlineLexerRulesBT(lex.backtracking)
                   : lex.backtracking,
     backtracking.length > 0-}
var tmBacktracking = []int{
${for i in [0, backtracking.length/2-1]-}
	${backtracking[i*2]}, ${backtracking[i*2+1]}, // in ${
	      rule = syntax.lexerRules[lex.backtracking[i*2]-2], rule.symbol.name-}
${if rule.symbol.rules.size() > 1}: /${rule.regexp}/${end}
${end-}
}
${end-}
${end}

${template lexerAction($, symbol)-}
${if self->isSpace()-}
		space = true
${end-}
${if codeTemplate-}
		${eval codeTemplate}
${end-}
${end}

${cached query runeClassType() = lex.nchars < 256 ? 'uint8' : lex.nchars < 65536 ? 'uint16' : 'int32' }

${cached query isSpace() = self.kindAsText == 'space' && !context.opts.reportTokens.exists(tok|tok == self.symbol)}

${cached query classRules() = syntax.lexerRules.select(x|x.kindAsText == 'class')}

${cached query classHasInstances() = self->classInstances().size() > 0 }

${cached query classInstances() = context.syntax.lexerRules.select(x|x.classRule && x.classRule == self)}

${cached query rangeSwitchSize() = util.rangeSwitchSize(self->classInstances().length)}

${cached query classRuleName() = util.uniqueId(util.toFirstUpper(self.symbol.id), '__classrule__')}

${cached query stateId() = 'State' + util.uniqueId(util.toCamelCase(self.nameText.replace('-', '_'), true), '__states__')}

${cached query canInlineLexerRules() = syntax.canInlineLexerRules}

${template onBeforeNext}${end}
${template onAfterNext}${end}

${template onBeforeLexer}${end}
${template onAfterLexer}${end}

${template stateVars}${end}
${template initStateVars}${end}

${cached query useCustomMap() = self->classRules().exists(it|it->classHasInstances())}

${cached query backupVar() = self->canInlineLexerRules() ? 'backupToken' : 'backupRule'}