// generated by Textmapper; DO NOT EDIT

package test

import (
	"strconv"
	"strings"
	"unicode/utf8"

	"github.com/inspirer/textmapper/parsers/test/token"
)

// Lexer states.
const (
	StateInitial     = 0
	StateInMultiLine = 1
)

// Lexer uses a generated DFA to scan through a utf-8 encoded input string. If
// the string starts with a BOM character, it gets skipped.
type Lexer struct {
	source string

	ch          rune // current character, -1 means EOI
	offset      int  // character offset
	tokenOffset int  // last token byte offset
	scanOffset  int  // scanning offset
	value       interface{}

	State int // lexer state, modifiable
}

var bomSeq = "\xef\xbb\xbf"

// Init prepares the lexer l to tokenize source by performing the full reset
// of the internal state.
func (l *Lexer) Init(source string) {
	l.source = source

	l.ch = 0
	l.offset = 0
	l.tokenOffset = 0
	l.State = 0

	if strings.HasPrefix(source, bomSeq) {
		l.offset += len(bomSeq)
	}

	l.rewind(l.offset)
}

// Next finds and returns the next token in l.source. The source end is
// indicated by Token.EOI.
//
// The token text can be retrieved later by calling the Text() method.
func (l *Lexer) Next() token.Token {
	var commentOffset, commentDepth int
restart:
	l.tokenOffset = l.offset

	state := tmStateMap[l.State]
	hash := uint32(0)
	backupRule := -1
	var backupOffset int
	backupHash := hash
	for state >= 0 {
		var ch int
		if uint(l.ch) < tmRuneClassLen {
			ch = int(tmRuneClass[l.ch])
		} else if l.ch < 0 {
			state = int(tmLexerAction[state*tmNumClasses])
			if state > tmFirstRule && state < 0 {
				state = (-1 - state) * 2
				backupRule = tmBacktracking[state]
				backupOffset = l.offset
				backupHash = hash
				state = tmBacktracking[state+1]
			}
			continue
		} else {
			ch = mapRune(l.ch)
		}
		state = int(tmLexerAction[state*tmNumClasses+ch])
		if state > tmFirstRule {
			if state < 0 {
				state = (-1 - state) * 2
				backupRule = tmBacktracking[state]
				backupOffset = l.offset
				backupHash = hash
				state = tmBacktracking[state+1]
			}
			hash = hash*uint32(31) + uint32(l.ch)

			// Scan the next character.
			// Note: the following code is inlined to avoid performance implications.
			l.offset = l.scanOffset
			if l.offset < len(l.source) {
				r, w := rune(l.source[l.offset]), 1
				if r >= 0x80 {
					// not ASCII
					r, w = utf8.DecodeRuneInString(l.source[l.offset:])
				}
				l.scanOffset += w
				l.ch = r
			} else {
				l.ch = -1 // EOI
			}
		}
	}

	rule := tmFirstRule - state
recovered:
	switch rule {
	case 4:
		hh := hash & 15
		switch hh {
		case 2:
			if hash == 0xc32 && "as" == l.source[l.tokenOffset:l.offset] {
				rule = 12
				break
			}
			if hash == 0x364492 && "test" == l.source[l.tokenOffset:l.offset] {
				rule = 8
				break
			}
		case 7:
			if hash == 0x5b098c7 && "decl1" == l.source[l.tokenOffset:l.offset] {
				rule = 9
				break
			}
		case 8:
			if hash == 0x5b098c8 && "decl2" == l.source[l.tokenOffset:l.offset] {
				rule = 10
				break
			}
			if hash == 0x18ac8 && "f_a" == l.source[l.tokenOffset:l.offset] {
				rule = 31
				break
			}
		case 9:
			if hash == 0x2f8d39 && "else" == l.source[l.tokenOffset:l.offset] {
				rule = 14
				break
			}
			if hash == 0x300c59 && "foo_" == l.source[l.tokenOffset:l.offset] {
				rule = 30
				break
			}
		case 12:
			if hash == 0x2fb09c && "eval" == l.source[l.tokenOffset:l.offset] {
				rule = 11
				break
			}
		case 13:
			if hash == 0xd1d && "if" == l.source[l.tokenOffset:l.offset] {
				rule = 13
				break
			}
		}
	case 35:
		hh := hash & 7
		switch hh {
		case 4:
			if hash == 0x2a762c && "Zfoo" == l.source[l.tokenOffset:l.offset] {
				rule = 37
				break
			}
		}
	}

	tok := tmToken[rule]
	var space bool
	switch rule {
	case 0:
		if backupRule >= 0 {
			rule = backupRule
			hash = backupHash
			l.rewind(backupOffset)
		} else if l.offset == l.tokenOffset {
			l.rewind(l.scanOffset)
		}
		if rule != 0 {
			goto recovered
		}
	case 2: // WhiteSpace: /[ \t\r\n\x00]/
		space = true
	case 6: // IntegerConstant: /[0-9]+/
		{
			l.value = mustParseInt(l.Text())
		}
	case 39: // MultiLineComment: /\/\*/
		{
			l.State = StateInMultiLine
			commentOffset = l.tokenOffset
			commentDepth = 0
			space = true
		}
	case 40: // invalid_token: /{eoi}/
		{
			l.tokenOffset = commentOffset
			l.State = StateInitial
		}
	case 41: // MultiLineComment: /\/\*/
		{
			commentDepth++
			space = true
		}
	case 42: // MultiLineComment: /\*\//
		{
			if commentDepth == 0 {
				space = false
				l.tokenOffset = commentOffset
				l.State = StateInitial
				break
			}
			space = true
			commentDepth--
		}
	case 43: // WhiteSpace: /[^\/*]+|[*\/]/
		space = true
		{
			space = true
		}
	}
	if space {
		goto restart
	}
	return tok
}

// Pos returns the start and end positions of the last token returned by Next().
func (l *Lexer) Pos() (start, end int) {
	start = l.tokenOffset
	end = l.offset
	return
}

// Text returns the substring of the input corresponding to the last token.
func (l *Lexer) Text() string {
	return l.source[l.tokenOffset:l.offset]
}

// Value returns the value associated with the last returned token.
func (l *Lexer) Value() interface{} {
	return l.value
}

// rewind can be used in lexer actions to accept a portion of a scanned token, or to include
// more text into it.
func (l *Lexer) rewind(offset int) {
	// Scan the next character.
	l.scanOffset = offset
	l.offset = offset
	if l.offset < len(l.source) {
		r, w := rune(l.source[l.offset]), 1
		if r >= 0x80 {
			// not ASCII
			r, w = utf8.DecodeRuneInString(l.source[l.offset:])
		}
		l.scanOffset += w
		l.ch = r
	} else {
		l.ch = -1 // EOI
	}
}

func mustParseInt(s string) int {
	i, err := strconv.Atoi(s)
	if err != nil {
		panic(`lexer internal error: ` + err.Error())
	}
	return i
}
