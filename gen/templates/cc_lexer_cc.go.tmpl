{{ template "header" . -}}
{{block "lexerIncludes" . -}}
#include "lexer.h"

#include "absl/log/log.h"
#include "absl/strings/match.h"
{{end}}
namespace {{.Options.Namespace}} {
{{block "onBeforeLexer" .}}{{end -}}
{{template "lexerTables" .}}
{{template "decodeRune" .}}
{{template "lexerCtor" .}}
{{template "lexerNext" .}}
{{template "lexerRewind" .}}
{{- block "onAfterLexer" .}}{{end}}
}  // namespace {{.Options.Namespace}}
{{/**/}}

{{- define "lexerTables" -}}
namespace {
constexpr int tmNumClasses = {{.Lexer.Tables.NumSymbols}};

{{$runeType := bits .Lexer.Tables.NumSymbols -}}
{{if gt .Lexer.Tables.LastMapEntry.Start 2048 -}}
// NOT TRANSLATED
type mapRange struct {
	lo         rune
	hi         rune
	defaultVal uint{{$runeType}}
	val        []uint{{$runeType}}
}

func mapRune(c rune) int {
	lo := 0
	hi := len(tmRuneRanges)
	for lo < hi {
		m := lo + (hi-lo)/2
		r := tmRuneRanges[m]
		if c < r.lo {
			hi = m
		} else if c >= r.hi {
			lo = m + 1
		} else {
			i := int(c - r.lo)
			if i < len(r.val) {
				return int(r.val[i])
			}
			return int(r.defaultVal)
		}
	}
	return {{.Lexer.Tables.LastMapEntry.Target}}
}

// Latin-1 characters.
var tmRuneClass = []uint{{$runeType}}{
{{- int_array (.Lexer.Tables.SymbolArr 256) "\t" 79 -}}
}

const tmRuneClassLen = 256
const tmFirstRule = {{.Lexer.Tables.ActionStart}}

var tmRuneRanges = []mapRange{
{{range .Lexer.Tables.CompressedMap 256}}	{ {{- .Lo}}, {{.Hi}}, {{.DefaultVal}}, {{if .Vals}}[]uint{{$runeType}}{
{{- int_array .Vals "\t\t" 78}}	}{{else}}nil{{end -}} },
{{end -}}
}

// END OF NOT TRANSLATED
{{else -}}
{{ $runeArr := .Lexer.Tables.SymbolArr 0 -}}
constexpr uint{{$runeType}}_t tmRuneClass[] = {
{{- int_array (.Lexer.Tables.SymbolArr 256) "\t" 79 -}}
};

constexpr int tmRuneClassLen = {{len $runeArr}};
constexpr int tmFirstRule = {{.Lexer.Tables.ActionStart}};

{{end -}}
constexpr int tmStateMap[] = {
{{- int_array .Lexer.Tables.StateMap "\t" 79 -}}
};

{{if .Lexer.RuleToken -}}
constexpr Token tmToken[] = {
{{- int_array .Lexer.RuleToken "\t" 79 -}}
};

{{end -}}
constexpr int{{bits_per_element .Lexer.Tables.Dfa}}_t tmLexerAction[] = {
{{- int_array .Lexer.Tables.Dfa "\t" 79 -}}
};

{{- if .Lexer.Tables.Backtrack}}

constexpr int tmBacktracking[] = {
{{- range .Lexer.Tables.Backtrack}}
	{{.Action}}, {{.NextState}},{{if .Details}} // {{.Details}}{{end}}
{{- end}}
};
{{- end}}
}  // namespace
{{end -}}

{{- define "decodeRune" -}}
constexpr uint32_t runeErr = 0xfffd;

inline uint32_t decodeRune(std::string_view input, int64_t& offset) {
  uint8_t b0 = input[offset++];
  if (b0 < 0x80) {
    return b0;  // ASCII
  }
  uint8_t head = (b0 >> 3) & 0xf;
  int sz =
      (0x3a550000 >> (head * 2)) & 3;  // 0b10xx -> 1, 0b110x -> 2, 0b1110 -> 3
  if (sz == 0 || offset + sz > input.size()) {
    return runeErr;
  }
  uint8_t b1 = input[offset++];
  if (b1 < 0x80 || b1 >= 0xc0) {
    return runeErr;
  }
  if (sz == 1) {
    return (static_cast<uint32_t>(b0) & 0x1F) << 6 | (b1 & 0x3F);
  }
  uint8_t b2 = input[offset++];
  if (b2 < 0x80 || b2 >= 0xc0) {
    return runeErr;
  }
  if (sz == 2) {
    return (static_cast<uint32_t>(b0) & 0xF) << 12 |
           (static_cast<uint32_t>(b1) & 0x3F) << 6 | (b2 & 0x3F);
  }
  uint8_t b3 = input[offset++];
  if (b3 < 0x80 || b3 >= 0xc0) {
    return runeErr;
  }
  return (static_cast<uint32_t>(b0) & 0x7) << 18 |
         (static_cast<uint32_t>(b1) & 0x3F) << 12 |
         (static_cast<uint32_t>(b2) & 0x3F) << 6 | (b3 & 0x3F);
}
{{end -}}

{{- define "lexerCtor" -}}
Lexer::Lexer(absl::string_view input_source) {
  source_ = input_source;
  if (absl::StartsWith(source_, bomSeq)) {
    offset_ += bomSeq.size();
  }
  Rewind(offset_);
}
{{end -}}

{{- define "lexerNext" -}}
Token Lexer::Next() {
{{ block "onBeforeNext" .}}{{end -}}
{{ $spaceRules := .SpaceActions -}}
{{ if or $spaceRules .Lexer.RuleToken -}}
restart:
{{ end -}}
{{ if .Options.TokenLine}}  token_line_ = line_;
{{ end -}}
{{ if .Options.TokenColumn}}  token_col_ = offset_ - line_offset_ + 1;
{{ end -}}
{{/**/}}  token_offset_ = offset_;

  int state = tmStateMap[start_state_];
{{- if .Lexer.ClassActions}}
  uint32_t hash = 0;
{{- end}}
{{- if .Lexer.Tables.Backtrack}}
{{- if .Lexer.RuleToken}}
  int backupRule = -1;
{{- else}}
  Token backupToken = Token::EOI;
{{- end}}
  uint64_t backupOffset;
{{- if .Lexer.ClassActions}}
  uint32_t backupHash = hash;
{{- end}}
{{- end}}
  while (state >= 0) {
    int curr_class;
    if (input_rune_ < 0) {
      state = tmLexerAction[state * tmNumClasses];
{{- if .Lexer.Tables.Backtrack}}
      if (state > tmFirstRule && state < 0) {
        state = (-1 - state) * 2;
{{- if .Lexer.RuleToken}}
        backupRule = tmBacktracking[state];
{{- else}}
        backupToken = Token(tmBacktracking[state]);
{{- end}}
        backupOffset = offset_;
{{- if .Lexer.ClassActions}}
        backupHash = hash;
{{- end}}
        state = tmBacktracking[state + 1];
      }
{{- end}}
      continue;
    } else if (input_rune_ < tmRuneClassLen) {
      curr_class = tmRuneClass[input_rune_];
    } else {
{{- if gt .Lexer.Tables.LastMapEntry.Start 2048}}
      curr_class = mapRune(l.ch);
{{- else}}
      curr_class = {{.Lexer.Tables.LastMapEntry.Target}};
{{- end}}
    }
    state = tmLexerAction[state * tmNumClasses + curr_class];
    if (state > tmFirstRule) {
{{- if .Lexer.Tables.Backtrack}}
      if (state < 0) {
        state = (-1 - state) * 2;
        backupToken = Token(tmBacktracking[state]);
        backupOffset = offset_;
{{- if .Lexer.ClassActions}}
        backupHash = hash;
{{- end}}
        state = tmBacktracking[state + 1];
      }
{{- end}}
{{- if .Lexer.ClassActions}}
      hash = hash * 31 + static_cast<uint32_t>(input_rune_);
{{end}}
{{- if .Options.TokenLine}}
      if (input_rune_ == '\n') {
        line_++;
{{- if or .Options.TokenLineOffset .Options.TokenColumn}}
        line_offset_ = offset_;
{{- end}}
      }
{{end}}
      // Scan the next character.
      offset_ = scan_offset_;
      if (offset_ < source_.size()) {
        input_rune_ = decodeRune(source_, scan_offset_);
      } else {
        input_rune_ = -1;
      }
    }
  }

{{if .Lexer.RuleToken}}
  int rule = tmFirstRule - state;
{{- else}}
  Token tok = Token(tmFirstRule - state);
{{- end}}
{{- if .Lexer.Tables.Backtrack}}
recovered:
{{- end}}
{{- if .Lexer.ClassActions}}
  switch ({{if .Lexer.RuleToken}}rule{{else}}tok{{end}}) {
{{- range .Lexer.ClassActions}}
{{- if $.Lexer.RuleToken}}
	case {{sum .Action 2}}:
{{- else}}
	case Token::{{(index $.Syms .Action).ID}}: {
{{- end}}
{{- with string_switch .Custom }}
      uint32_t hh = hash & {{.Mask}};
      switch (hh) {
{{- range .Cases}}
      case {{.Value}}:
{{- range .Subcases}}
        if (hash == {{hex .Hash}} && {{quote .Str}} == Text()) {
{{- if $.Lexer.RuleToken}}
          rule = {{sum .Action 2}};
{{- else}}
          tok = Token::{{(index $.Syms .Action).ID}};
{{- end}}
          break;
        }
{{- end}}
        break;
{{- end}}
      }
      break;
{{- end}}
{{- end}}
    }
    default:
      break;
  }
{{- end}}
{{- if .Lexer.RuleToken}}
// TODO NOT TRANSLATED

	tok := tmToken[rule]
	var space bool
{{- if .Lexer.Actions}}
	switch rule {
	case 0:
{{- template "handleInvalidToken" .}}
{{- range .Lexer.Actions}}
	case {{sum .Action 2}}:{{if .Comments}} // {{join .Comments ", "}}{{end}}
{{- if .Space }}
		space = true
{{- end}}
{{- if .Code }}
{{lexer_action .Code}}
{{- end}}
{{- end}}
	}
{{- else}}
	if rule == 0 {
{{- template "handleInvalidToken" .}}
	}
{{- end}}
	if space {
		goto restart
	}
// END OF NOT TRANSLATED
{{- else}}
  switch (tok) {
    case Token::{{(index $.Syms .Lexer.InvalidToken).ID}}:
{{- template "handleInvalidToken" .}}
      break;
    case Token(8):
      goto restart;
    default:
      break;
  }
{{- end}}
{{ block "onAfterNext" .}}{{end -}}
{{/**/}}	return tok;
}
{{end -}}

{{- define "lexerRewind" -}}
void Lexer::Rewind(int64_t rewind_offset) {
{{- if .Options.TokenLine}}
  if (rewind_offset < offset_) {
    for (int64_t i = rewind_offset; i < offset_; ++i) {
      if (source_[i] == '\n') {
        line_--;
      }
    }
  } else {
    if (rewind_offset > source_.size()) {
      rewind_offset = source_.size();
    }
    for (int64_t i = offset_; i < rewind_offset; ++i) {
      if (source_[i] == '\n') {
        line_++;
      }
    }
  }
{{- if or .Options.TokenLineOffset .Options.TokenColumn}}
  line_offset_ = ;// TODO l.lineOffset = 1 + "strings".LastIndexByte(l.source[:offset], '\n')
{{- end}}
{{end}}
  // Scan the next character.
  scan_offset_ = rewind_offset;
  offset_ = rewind_offset;
  if (offset_ < source_.size()) {
    input_rune_ = decodeRune(source_, scan_offset_);
  } else {
    input_rune_ = -1;  // Invalid rune for end of input
  }
}
{{end -}}

{{- define "handleInvalidToken" -}}
{{if .Lexer.Tables.Backtrack}}
      if ({{if .Lexer.RuleToken}}backupRule{{else}}static_cast<int>(backupToken){{end}} >= 0) {
{{- if .Lexer.RuleToken}}
        rule = backupRule;
{{- else}}
        tok = backupToken;
{{- end}}
{{- if .Lexer.ClassActions}}
        hash = backupHash;
{{- end}}
        Rewind(backupOffset);
      } else if (offset_ == token_offset_) {
        Rewind(scan_offset_);
      }
{{- if .Lexer.RuleToken}}
      if (rule != 0) {
{{- else}}
      if (tok != Token::{{(index $.Syms .Lexer.InvalidToken).ID}}) {
{{- end}}
        goto recovered;
      }
{{- else}}
      if (offset_ == token_offset_) {
        Rewind(scan_offset_);
      }
{{- end -}}
{{end}}
