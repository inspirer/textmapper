{{ template "header" . -}}
package {{short_pkg .Options.Package}}

{{ if gt (len .Lexer.StartConditions) 1 -}}
// Lexer states.
const (
{{ range $index, $el := .Lexer.StartConditions -}}
	State{{title .}} = {{$index}}
{{ end -}}
)

{{ end -}}
{{ block "onBeforeLexer" .}}{{end}}
{{ template "lexerType" .}}
{{ template "lexerInit" .}}
{{ template "lexerNext" .}}
{{ template "lexerPos" .}}
{{ if .Options.TokenLine -}}
{{ template "lexerLine" .}}
{{ end -}}
{{ if .Options.TokenColumn -}}
{{ template "lexerColumn" .}}
{{ end -}}
{{ template "lexerText" .}}
{{ template "lexerValue" .}}
{{ template "lexerRewind" .}}
{{ block "onAfterLexer" .}}{{end -}}

{{- define "lexerType" -}}
// Lexer uses a generated DFA to scan through a utf-8 encoded input string. If
// the string starts with a BOM character, it gets skipped.
type Lexer struct {
	source string

	ch          rune // current character, -1 means EOI
	offset      int  // character offset
	tokenOffset int  // last token byte offset
{{ if .Options.TokenLine -}}
	line        int  // current line number (1-based)
	tokenLine   int  // last token line
{{ end -}}
{{ if or .Options.TokenLineOffset .Options.TokenColumn -}}
	lineOffset  int  // current line offset
{{ end -}}
{{ if .Options.TokenColumn -}}
	tokenColumn int  // last token column (in bytes)
{{ end -}}
	scanOffset  int  // scanning offset
	value       interface{}

{{ if gt (len .Lexer.StartConditions) 1 -}}
	State int // lexer state, modifiable
{{ end -}}
{{ block "sharedStateVars" .}}{{end -}}
{{ block "stateVars" .}}{{end -}}
}
{{end -}}

{{- define "lexerInit" -}}
var bomSeq = "\xef\xbb\xbf"

// Init prepares the lexer l to tokenize source by performing the full reset
// of the internal state.
func (l *Lexer) Init(source string) {
	l.source = source

	l.ch = 0
	l.offset = 0
	l.tokenOffset = 0
{{ if .Options.TokenLine -}}
	l.line = 1
	l.tokenLine = 1
{{ end -}}
{{ if or .Options.TokenLineOffset .Options.TokenColumn -}}
	l.lineOffset = 0
{{ end -}}
{{ if .Options.TokenColumn -}}
	l.tokenColumn = 1
{{ end -}}
{{ if gt (len .Lexer.StartConditions) 1 -}}
	l.State = 0
{{ end -}}
{{ block "sharedInitStateVars" .}}{{end -}}
{{ block "initStateVars" .}}{{end}}
	if "strings".HasPrefix(source, bomSeq) {
		l.offset += len(bomSeq)
	}

	l.rewind(l.offset)
}
{{end -}}

{{- define "lexerNext" -}}
// Next finds and returns the next token in l.source. The source end is
// indicated by Token.EOI.
//
// The token text can be retrieved later by calling the Text() method.
func (l *Lexer) Next() {{template "tokenType" .}} {
{{ block "onBeforeNext" .}}{{end -}}
{{ $spaceRules := .SpaceActions -}}
{{ if or $spaceRules .Lexer.RuleToken -}}
restart:
{{ end -}}
{{ if .Options.TokenLine -}}
	l.tokenLine = l.line
{{ end -}}
{{ if .Options.TokenColumn -}}
	l.tokenColumn = l.offset-l.lineOffset+1
{{ end -}}
	l.tokenOffset = l.offset

	state := {{ if gt (len .Lexer.StartConditions) 1 }}tmStateMap[l.State]{{else}}{{index .Lexer.Tables.StateMap 0}}{{end}}
{{ if .Lexer.ClassActions -}}
	hash := uint32(0)
{{ end -}}
{{ if .Lexer.Tables.Backtrack -}}
	backup{{if .Lexer.RuleToken}}Rule{{else}}Token{{end}} := -1
	var backupOffset int
{{ if .Lexer.ClassActions -}}
	backupHash := hash
{{ end -}}
{{ end -}}
	for state >= 0 {
		var ch int
		if uint(l.ch) < tmRuneClassLen {
			ch = int(tmRuneClass[l.ch])
		} else if l.ch < 0 {
			state = int(tmLexerAction[state*tmNumClasses])
{{ if .Lexer.Tables.Backtrack -}}
			if state > tmFirstRule && state < 0 {
				state = (-1 - state) * 2
				backup{{if .Lexer.RuleToken}}Rule{{else}}Token{{end}} = tmBacktracking[state]
				backupOffset = l.offset
{{ if .Lexer.ClassActions -}}
				backupHash = hash
{{ end -}}
				state = tmBacktracking[state+1]
			}
{{ end -}}
			continue
		} else {
{{ if gt .Lexer.Tables.LastMapEntry.Start 2048 -}}
			ch = mapRune(l.ch)
{{ else -}}
			ch = {{.Lexer.Tables.LastMapEntry.Target}}
{{ end -}}
		}
		state = int(tmLexerAction[state*tmNumClasses+ch])
		if state > tmFirstRule {
{{ if .Lexer.Tables.Backtrack -}}
			if state < 0 {
				state = (-1 - state) * 2
				backup{{if .Lexer.RuleToken}}Rule{{else}}Token{{end}} = tmBacktracking[state]
				backupOffset = l.offset
{{ if .Lexer.ClassActions -}}
				backupHash = hash
{{ end -}}
				state = tmBacktracking[state+1]
			}
{{ end -}}
{{ if .Lexer.ClassActions -}}
			hash = hash*uint32(31) + uint32(l.ch)
{{ end -}}
{{ if .Options.TokenLine -}}
			if l.ch == '\n' {
				l.line++
{{ if or .Options.TokenLineOffset .Options.TokenColumn -}}
				l.lineOffset = l.offset
{{ end -}}
			}
{{ end -}}

			// Scan the next character.
			// Note: the following code is inlined to avoid performance implications.
			l.offset = l.scanOffset
			if l.offset < len(l.source) {
				r, w := rune(l.source[l.offset]), 1
				if r >= 0x80 {
					// not ASCII
					r, w = "unicode/utf8".DecodeRuneInString(l.source[l.offset:])
				}
				l.scanOffset += w
				l.ch = r
			} else {
				l.ch = -1 // EOI
			}
		}
	}

{{if .Lexer.RuleToken -}}
	rule := tmFirstRule - state
{{ else -}}
	tok := {{template "tokenType" .}}(tmFirstRule - state)
{{ end -}}
{{ if .Lexer.Tables.Backtrack -}}
recovered:
{{ end -}}
{{ if .Lexer.ClassActions -}}
	switch {{if .Lexer.RuleToken}}rule{{else}}tok{{end}} {
{{ range .Lexer.ClassActions -}}
{{ if $.Lexer.RuleToken -}}
	case {{sum .Action 2}}:
{{ else -}}
	case {{template "tokenPkg" $}}{{(index $.Syms .Action).ID}}:
{{ end -}}
{{ with string_switch .Custom -}}
		hh := hash & {{.Mask}}
		switch hh {
{{ range .Cases -}}
		case {{.Value}}:
{{ range .Subcases -}}
			if hash == {{hex .Hash}} && {{quote .Str}} == l.source[l.tokenOffset:l.offset] {
{{ if $.Lexer.RuleToken -}}
				rule = {{sum .Action 2}}
{{ else -}}
				tok = {{template "tokenPkg" $}}{{(index $.Syms .Action).ID}}
{{ end -}}
				break
			}
{{ end -}}
{{ end }}{{/* .Cases */ -}}
		}
{{ end -}}
{{ end }}{{/* .Lexer.ClassActions */ -}}
	}
{{ end -}}
{{ if .Lexer.RuleToken -}}

	tok := tmToken[rule]
	var space bool
{{ if .Lexer.Actions -}}
	switch rule {
	case 0:
{{ template "handleInvalidToken" . -}}
{{ range .Lexer.Actions -}}
	case {{sum .Action 2}}:{{if .Comments}} // {{join .Comments ", "}}{{end}}
{{ if .Space -}}
		space = true
{{ end -}}
{{ if .Code -}}
{{lexer_action .Code}}
{{ end -}}
{{ end }}{{/* .Lexer.Actions */ -}}
	}
{{ else -}}
	if rule == 0 {
{{ template "handleInvalidToken" . -}}
	}
{{ end -}}
	if space {
		goto restart
	}
{{ else -}}
	switch tok {
	case {{template "tokenPkg" .}}{{(index $.Syms .Lexer.InvalidToken).ID}}:
{{ template "handleInvalidToken" . -}}
{{ if $spaceRules -}}
	case {{range $i, $val := $spaceRules}}{{if gt $i 0}}, {{end}}{{$val}}{{end}}:
		goto restart
{{ end -}}
	}
{{ end -}}
{{ block "onAfterNext" .}}{{end -}}
	return tok
}
{{end -}}

{{- define "lexerPos" -}}
// Pos returns the start and end positions of the last token returned by Next().
func (l *Lexer) Pos() (start, end int) {
	start = l.tokenOffset
	end = l.offset
	return
}
{{end -}}

{{- define "lexerLine" -}}
// Line returns the line number of the last token returned by Next() (1-based).
func (l *Lexer) Line() int {
	return l.tokenLine
}
{{end -}}

{{- define "lexerColumn" -}}
// Column returns the column of the last token returned by Next() (in bytes, 1-based).
func (l *Lexer) Column() int {
	return l.tokenColumn
}
{{end -}}

{{- define "lexerText" -}}
// Text returns the substring of the input corresponding to the last token.
func (l *Lexer) Text() string {
	return l.source[l.tokenOffset:l.offset]
}
{{end -}}

{{- define "lexerValue" -}}
// Value returns the value associated with the last returned token.
func (l *Lexer) Value() interface{} {
	return l.value
}
{{end -}}

{{- define "lexerRewind" -}}
// rewind can be used in lexer actions to accept a portion of a scanned token, or to include
// more text into it.
func (l *Lexer) rewind(offset int) {
{{ if .Options.TokenLine -}}
	if offset < l.offset {
		l.line -= "strings".Count(l.source[offset:l.offset], "\n")
	} else {
		if offset > len(l.source) {
			offset = len(l.source)
		}
		l.line += "strings".Count(l.source[l.offset:offset], "\n")
	}
{{ if or .Options.TokenLineOffset .Options.TokenColumn -}}
	l.lineOffset = 1 + "strings".LastIndexByte(l.source[:offset], '\n')
{{ end -}}

{{ end -}}
	// Scan the next character.
	l.scanOffset = offset
	l.offset = offset
	if l.offset < len(l.source) {
		r, w := rune(l.source[l.offset]), 1
		if r >= 0x80 {
			// not ASCII
			r, w = "unicode/utf8".DecodeRuneInString(l.source[l.offset:])
		}
		l.scanOffset += w
		l.ch = r
	} else {
		l.ch = -1 // EOI
	}
}
{{end -}}

{{- define "handleInvalidToken" -}}
{{ if .Lexer.Tables.Backtrack -}}
		if backup{{if .Lexer.RuleToken}}Rule{{else}}Token{{end}} >= 0 {
{{ if .Lexer.RuleToken -}}
			rule = backupRule
{{ else -}}
			tok = {{template "tokenType" .}}(backupToken)
{{ end -}}
{{ if .Lexer.ClassActions -}}
			hash = backupHash
{{ end -}}
			l.rewind(backupOffset)
		} else if l.offset == l.tokenOffset {
			l.rewind(l.scanOffset)
		}
{{ if .Lexer.RuleToken -}}
		if rule != 0 {
{{ else -}}
		if tok != {{template "tokenPkg" .}}{{(index $.Syms .Lexer.InvalidToken).ID}} {
{{ end -}}
			goto recovered
		}
{{ else -}}
		if l.offset == l.tokenOffset {
			l.rewind(l.scanOffset)
		}
{{ end -}}
{{ end -}}
